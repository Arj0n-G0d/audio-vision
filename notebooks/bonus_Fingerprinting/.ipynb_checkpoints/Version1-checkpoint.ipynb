{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c529c7b9-4bea-40d7-b1af-1cf0227a2a2d",
   "metadata": {},
   "source": [
    "# Table Of Contents\n",
    "1. Libraries and Dependencies\n",
    "2. Preprocessing of Dataset\n",
    "    * Denoising the Audio Dataset\n",
    "    * Making positive Negative pairs\n",
    "3. Training the Model\n",
    "4. Training using Twin Neural Network Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d349a65-2c09-49b4-8666-8423570575fc",
   "metadata": {},
   "source": [
    "# Libraries and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88af979-19f2-464f-94db-0547bf587095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import librosa as lb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deee83d9-9815-417d-b8b5-4a964d85f30a",
   "metadata": {},
   "source": [
    "# Preprocessing the Dataset\n",
    "**Making Positive Negative Pairs of Dataset to work on it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5285736f-0689-45ad-9ada-480f5f732659",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import csv\n",
    "\n",
    "# Base directory of your dataset\n",
    "base_dir = \"/kaggle/input/voxceleb1-audio-wav-files-for-india-celebrity/vox1_indian/content/vox_indian\"\n",
    "\n",
    "# Function to get all audio file paths grouped by ID\n",
    "def get_audio_paths(base_dir):\n",
    "    id_folders = [os.path.join(base_dir, id_folder) for id_folder in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, id_folder))]\n",
    "    audio_dict = {}\n",
    "    for id_folder in id_folders:\n",
    "        id_name = os.path.basename(id_folder)\n",
    "        audio_files = []\n",
    "        for root, _, files in os.walk(id_folder):\n",
    "            for file in files:\n",
    "                if file.endswith(\".wav\"):\n",
    "                    audio_files.append(os.path.join(root, file))\n",
    "        audio_dict[id_name] = audio_files\n",
    "    return audio_dict\n",
    "\n",
    "# Generate positive and negative pairs\n",
    "def generate_pairs(audio_dict, num_pairs=5000):\n",
    "    positive_pairs = []\n",
    "    negative_pairs = []\n",
    "    \n",
    "    # Generate positive pairs\n",
    "    for id_name, audio_files in audio_dict.items():\n",
    "        if len(audio_files) > 1:  # At least two files to create a pair\n",
    "            positive_pairs.extend([(f1, f2, 1) for f1 in audio_files for f2 in audio_files if f1 != f2])\n",
    "    random.shuffle(positive_pairs)\n",
    "    positive_pairs = positive_pairs[:num_pairs]\n",
    "    \n",
    "    # Generate negative pairs\n",
    "    all_ids = list(audio_dict.keys())\n",
    "    while len(negative_pairs) < num_pairs:\n",
    "        id1, id2 = random.sample(all_ids, 2)\n",
    "        file1 = random.choice(audio_dict[id1])\n",
    "        file2 = random.choice(audio_dict[id2])\n",
    "        negative_pairs.append((file1, file2, 0))\n",
    "    \n",
    "    return positive_pairs, negative_pairs\n",
    "\n",
    "# Save pairs to CSV\n",
    "def save_pairs_to_csv(pairs, output_path):\n",
    "    with open(output_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['file1', 'file2', 'label'])  # Header\n",
    "        writer.writerows(pairs)\n",
    "\n",
    "# Main execution\n",
    "audio_dict = get_audio_paths(base_dir)\n",
    "positive_pairs, negative_pairs = generate_pairs(audio_dict, num_pairs=5000)\n",
    "\n",
    "# Combine and save\n",
    "all_pairs = positive_pairs + negative_pairs\n",
    "random.shuffle(all_pairs)\n",
    "output_csv_path = \"/kaggle/working/audio_pairs.csv\"\n",
    "save_pairs_to_csv(all_pairs, output_csv_path)\n",
    "\n",
    "print(f\"✅ Pairs saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249750f7-420b-4583-ba7d-d5b48b1b0993",
   "metadata": {},
   "source": [
    "**Denoising the Audio Dataset + Making Positive Negative Pairs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ec1162-396b-4478-9f9e-cd0a46c026fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import noisereduce as nr\n",
    "\n",
    "# Base directory of your dataset\n",
    "input_dir = \"/kaggle/input/voxceleb1-audio-wav-files-for-india-cele/vox1_indian/content/vox_indian\"\n",
    "output_dir = \"/kaggle/working/denoised_vox_indian\"\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Denoise function\n",
    "def denoise_audio(input_path, output_path):\n",
    "    try:\n",
    "        # Load the audio file\n",
    "        y, sr = librosa.load(input_path, sr=None)\n",
    "\n",
    "        # Estimate noise using the first 0.5 seconds of the audio\n",
    "        noise_part = y[:int(sr * 0.5)]\n",
    "        y_denoised = nr.reduce_noise(y=y, sr=sr, y_noise=noise_part)\n",
    "\n",
    "        # Save the denoised audio\n",
    "        sf.write(output_path, y_denoised, sr)\n",
    "        print(f\"✅ Denoised and saved: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing {input_path}: {e}\")\n",
    "\n",
    "# Process each ID folder\n",
    "for id_folder in os.listdir(input_dir):\n",
    "    id_path = os.path.join(input_dir, id_folder)\n",
    "\n",
    "    # Ensure it's a directory\n",
    "    if os.path.isdir(id_path):\n",
    "        # Create the output folder for the ID\n",
    "        output_id_folder = os.path.join(output_dir, id_folder)\n",
    "        os.makedirs(output_id_folder, exist_ok=True)\n",
    "\n",
    "        # Process each .wav file in the ID folder\n",
    "        for root, _, files in os.walk(id_path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".wav\"):\n",
    "                    input_file_path = os.path.join(root, file)\n",
    "                    output_file_path = os.path.join(output_id_folder, file)\n",
    "                    \n",
    "                    # Denoise and save the audio\n",
    "                    denoise_audio(input_file_path, output_file_path)\n",
    "\n",
    "print(f\"✅ All files have been denoised and saved in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609ed83-530a-4739-a45b-79f881016996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import csv\n",
    "\n",
    "# Path to the spectrograms folder\n",
    "input_dir = \"/kaggle/input/spectrograms\"\n",
    "\n",
    "# Output CSV file\n",
    "output_csv = \"/kaggle/working/spectrogram_pairs.csv\"\n",
    "\n",
    "# Step 1: Organize spectrograms by IDs\n",
    "spectrograms_by_id = {}\n",
    "for person_id in os.listdir(input_dir):\n",
    "    person_folder = os.path.join(input_dir, person_id)\n",
    "    if os.path.isdir(person_folder):\n",
    "        spectrograms_by_id[person_id] = [\n",
    "            os.path.join(person_folder, file)\n",
    "            for file in os.listdir(person_folder)\n",
    "            if file.endswith(\".png\")\n",
    "        ]\n",
    "\n",
    "# Step 2: Create Positive Pairs\n",
    "positive_pairs = []\n",
    "for person_id, files in spectrograms_by_id.items():\n",
    "    if len(files) > 1:  # Ensure there are at least 2 files to create pairs\n",
    "        positive_pairs.extend([\n",
    "            (files[i], files[j], 1)  # 1 indicates a positive pair\n",
    "            for i in range(len(files))\n",
    "            for j in range(i + 1, len(files))\n",
    "        ])\n",
    "\n",
    "# Shuffle and limit to 20,000 positive pairs\n",
    "random.shuffle(positive_pairs)\n",
    "positive_pairs = positive_pairs[:20000]\n",
    "\n",
    "# Step 3: Create Negative Pairs\n",
    "negative_pairs = []\n",
    "person_ids = list(spectrograms_by_id.keys())\n",
    "while len(negative_pairs) < 20000:\n",
    "    id1, id2 = random.sample(person_ids, 2)  # Select two different IDs\n",
    "    file1 = random.choice(spectrograms_by_id[id1])\n",
    "    file2 = random.choice(spectrograms_by_id[id2])\n",
    "    negative_pairs.append((file1, file2, 0))  # 0 indicates a negative pair\n",
    "\n",
    "# Step 4: Combine and Save Pairs\n",
    "all_pairs = positive_pairs + negative_pairs\n",
    "random.shuffle(all_pairs)  # Shuffle the pairs before saving\n",
    "\n",
    "# Save pairs to a CSV file\n",
    "with open(output_csv, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\"file1\", \"file2\", \"label\"])  # Header row\n",
    "    writer.writerows(all_pairs)\n",
    "\n",
    "print(f\"Pairs saved to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137de62d-f893-4e0b-bd4f-c6a5018804b3",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79acd88c-65cf-4dbd-99be-62197add609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import csv\n",
    "\n",
    "# Enable Mixed Precision\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# Parameters\n",
    "batch_size = 8\n",
    "csv_file = \"/kaggle/input/spectogram-pairs/spectrogram_pairs.csv\"\n",
    "input_shape = (224, 224, 1)\n",
    "total_samples = 40001  # Ensure we have enough samples\n",
    "\n",
    "# Ensure dataset generates enough samples per epoch\n",
    "steps_per_epoch = (total_samples // batch_size) + 1\n",
    "validation_samples = int(total_samples * 0.2)  # 20% for validation\n",
    "validation_steps = (validation_samples // batch_size) + 1  \n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, color_mode=\"grayscale\")  # Load as grayscale\n",
    "    img = img_to_array(img) / 255.0  # Normalize to [0,1]\n",
    "    return img.astype(\"float32\")  # Ensure TensorFlow compatibility\n",
    "\n",
    "# Function to create a generator\n",
    "def pair_generator(csv_file, repeat=False):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    while True:  # Loop indefinitely for training\n",
    "        for _, row in df.iterrows():\n",
    "            img1 = preprocess_image(row[\"file1\"])\n",
    "            img2 = preprocess_image(row[\"file2\"])\n",
    "            label = np.array(row[\"label\"], dtype=np.float32)  # Convert label to float32\n",
    "            yield (img1, img2), label\n",
    "        if not repeat:\n",
    "            break  # Stop after one pass for validation\n",
    "\n",
    "# Function to create a TensorFlow dataset\n",
    "def create_tf_dataset(csv_file, batch_size, repeat=True):\n",
    "    output_signature = (\n",
    "        (tf.TensorSpec(shape=(224, 224, 1), dtype=tf.float32),  \n",
    "         tf.TensorSpec(shape=(224, 224, 1), dtype=tf.float32)),  \n",
    "        tf.TensorSpec(shape=(), dtype=tf.float32)  \n",
    "    )\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: pair_generator(csv_file, repeat=repeat),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    \n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = create_tf_dataset(csv_file, batch_size, repeat=True)  # Train with repeat\n",
    "val_dataset = create_tf_dataset(csv_file, batch_size, repeat=False)  # No repeat for validation\n",
    "\n",
    "# Define the Siamese Model\n",
    "def build_siamese_model(input_shape):\n",
    "    base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "    input_a = Input(shape=input_shape)\n",
    "    input_b = Input(shape=input_shape)\n",
    "\n",
    "    # Stack grayscale images into 3 channels\n",
    "    stack_a = tf.keras.layers.Concatenate()([input_a, input_a, input_a])\n",
    "    stack_b = tf.keras.layers.Concatenate()([input_b, input_b, input_b])\n",
    "\n",
    "    feat_a = feature_extractor(stack_a)\n",
    "    feat_b = feature_extractor(stack_b)\n",
    "\n",
    "    l1_distance = Lambda(lambda tensors: tf.abs(tensors[0] - tensors[1]))([feat_a, feat_b])\n",
    "    output = Dense(1, activation=\"sigmoid\")(l1_distance)\n",
    "\n",
    "    model = Model(inputs=[input_a, input_b], outputs=output)\n",
    "    return model\n",
    "\n",
    "siamese_model = build_siamese_model(input_shape)\n",
    "\n",
    "# Compile the Model\n",
    "siamese_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Custom Callback for Epoch Tracking\n",
    "class SaveEpochMetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, output_csv_path):\n",
    "        super(SaveEpochMetricsCallback, self).__init__()\n",
    "        self.output_csv_path = output_csv_path\n",
    "        with open(self.output_csv_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"epoch\", \"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\"])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_metrics = [\n",
    "            epoch + 1,  # Epoch number\n",
    "            logs.get(\"loss\"),  # Training loss\n",
    "            logs.get(\"accuracy\"),  # Training accuracy\n",
    "            logs.get(\"val_loss\"),  # Validation loss\n",
    "            logs.get(\"val_accuracy\")  # Validation accuracy\n",
    "        ]\n",
    "        with open(self.output_csv_path, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(epoch_metrics)\n",
    "        print(f\"✅ Epoch {epoch + 1} metrics saved to {self.output_csv_path}\")\n",
    "\n",
    "# Path to save CSV\n",
    "epoch_metrics_csv = \"/kaggle/working/epoch_metrics.csv\"\n",
    "save_metrics_callback = SaveEpochMetricsCallback(output_csv_path=epoch_metrics_csv)\n",
    "\n",
    "# Train the model\n",
    "with tf.device('/GPU:0'):  # Ensure GPU usage\n",
    "    history = siamese_model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        steps_per_epoch=steps_per_epoch,  \n",
    "        validation_steps=validation_steps,  \n",
    "        epochs=10,\n",
    "        callbacks=[save_metrics_callback]\n",
    "    )\n",
    "\n",
    "# Save the model\n",
    "siamese_model.save(\"/kaggle/working/siamese_resnet_model.h5\")\n",
    "\n",
    "# Evaluate the Model\n",
    "with tf.device('/GPU:0'):\n",
    "    loss, accuracy = siamese_model.evaluate(val_dataset, steps=validation_steps)\n",
    "    print(f\"🎯 Validation Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cce003-97c1-464a-b33d-e7299fa0e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model using Twin Neural Networking Concept\n",
    "\n",
    "**Again Training the model using Twin Neural Networking Concept.For Referring to this Concept, [Do Check Here](https://www.mathworks.com/help/deeplearning/ug/train-twin-network-to-compare-images.html)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf96594b-fa8e-48a4-96b5-22ef29dd45a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import csv\n",
    "\n",
    "print(\"started\")\n",
    "\n",
    "# Enable Mixed Precision\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# Parameters\n",
    "batch_size = 8\n",
    "csv_file = \"/kaggle/input/spectogram-pairs/spectrogram_pairs.csv\"\n",
    "input_shape = (224, 224, 1)\n",
    "total_samples = 40001  # Ensure we have enough samples\n",
    "\n",
    "# Define dataset parameters\n",
    "steps_per_epoch = (total_samples // batch_size) + 1\n",
    "validation_samples = int(total_samples * 0.2)  # 20% for validation\n",
    "validation_steps = (validation_samples // batch_size) + 1  \n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, color_mode=\"grayscale\")  # Load as grayscale\n",
    "    img = img_to_array(img) / 255.0  # Normalize to [0,1]\n",
    "    return img.astype(\"float32\")  # Ensure TensorFlow compatibility\n",
    "\n",
    "# Function to create a generator\n",
    "def pair_generator(csv_file, repeat=False):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    while True:  # Loop indefinitely for training\n",
    "        for _, row in df.iterrows():\n",
    "            img1 = preprocess_image(row[\"file1\"])\n",
    "            img2 = preprocess_image(row[\"file2\"])\n",
    "            label = np.array(row[\"label\"], dtype=np.float32)  # Convert label to float32\n",
    "            yield (img1, img2), label\n",
    "        if not repeat:\n",
    "            break  # Stop after one pass for validation\n",
    "\n",
    "# Function to create a TensorFlow dataset\n",
    "def create_tf_dataset(csv_file, batch_size, repeat=True):\n",
    "    output_signature = (\n",
    "        (tf.TensorSpec(shape=(224, 224, 1), dtype=tf.float32),  \n",
    "         tf.TensorSpec(shape=(224, 224, 1), dtype=tf.float32)),  \n",
    "        tf.TensorSpec(shape=(), dtype=tf.float32)  \n",
    "    )\n",
    "\n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        lambda: pair_generator(csv_file, repeat=repeat),\n",
    "        output_signature=output_signature\n",
    "    )\n",
    "    \n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = create_tf_dataset(csv_file, batch_size, repeat=True)  # Train with repeat\n",
    "val_dataset = create_tf_dataset(csv_file, batch_size, repeat=False)  # No repeat for validation\n",
    "\n",
    "# Step 2: Build the Twin Neural Network\n",
    "def build_twin_network(input_shape):\n",
    "    base_model = ResNet50(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False  # Freeze ResNet weights\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    feature_extractor = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "    input_a = Input(shape=input_shape)\n",
    "    input_b = Input(shape=input_shape)\n",
    "\n",
    "    # Stack grayscale images into 3 channels\n",
    "    stack_a = tf.keras.layers.Concatenate()([input_a, input_a, input_a])\n",
    "    stack_b = tf.keras.layers.Concatenate()([input_b, input_b, input_b])\n",
    "\n",
    "    feat_a = feature_extractor(stack_a)\n",
    "    feat_b = feature_extractor(stack_b)\n",
    "\n",
    "    # Compute L2 distance\n",
    "    def euclidean_distance(vects):\n",
    "        x, y = vects\n",
    "        return tf.sqrt(tf.reduce_sum(tf.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "    distance = Lambda(euclidean_distance)([feat_a, feat_b])\n",
    "    output = Dense(1, activation=\"sigmoid\")(distance)  # Binary classification (similar/dissimilar)\n",
    "\n",
    "    model = Model(inputs=[input_a, input_b], outputs=output)\n",
    "    return model\n",
    "\n",
    "twin_model = build_twin_network(input_shape)\n",
    "\n",
    "# Step 3: Define Contrastive Loss Function\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    margin = 1.0\n",
    "    return tf.reduce_mean(y_true * tf.square(y_pred) + (1 - y_true) * tf.square(tf.maximum(margin - y_pred, 0)))\n",
    "\n",
    "# Compile the Model\n",
    "twin_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss=contrastive_loss,\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Custom Callback for Epoch Tracking with Printing\n",
    "class SaveAndPrintEpochMetrics(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, output_csv_path):\n",
    "        super(SaveAndPrintEpochMetrics, self).__init__()\n",
    "        self.output_csv_path = output_csv_path\n",
    "        with open(self.output_csv_path, mode='w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"epoch\", \"loss\", \"accuracy\", \"val_loss\", \"val_accuracy\"])\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_metrics = [\n",
    "            epoch + 1,  # Epoch number\n",
    "            logs.get(\"loss\"),  # Training loss\n",
    "            logs.get(\"accuracy\"),  # Training accuracy\n",
    "            logs.get(\"val_loss\"),  # Validation loss\n",
    "            logs.get(\"val_accuracy\")  # Validation accuracy\n",
    "        ]\n",
    "        # Save to CSV\n",
    "        with open(self.output_csv_path, mode='a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(epoch_metrics)\n",
    "\n",
    "        # Print the metrics after every epoch\n",
    "        print(f\"\\n🔹 Epoch {epoch + 1} Completed\")\n",
    "        print(f\"📉 Training Loss: {logs.get('loss'):.4f}\")\n",
    "        print(f\"✅ Training Accuracy: {logs.get('accuracy'):.4f}\")\n",
    "        print(f\"📊 Validation Loss: {logs.get('val_loss'):.4f}\")\n",
    "        print(f\"🎯 Validation Accuracy: {logs.get('val_accuracy'):.4f}\\n\")\n",
    "\n",
    "# Path to save CSV\n",
    "epoch_metrics_csv = \"/kaggle/working/epoch_metrics.csv\"\n",
    "save_print_callback = SaveAndPrintEpochMetrics(output_csv_path=epoch_metrics_csv)\n",
    "\n",
    "# Train the model\n",
    "with tf.device('/GPU:0'):  # Ensure GPU usage\n",
    "    history = twin_model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        steps_per_epoch=steps_per_epoch,  \n",
    "        validation_steps=validation_steps,  \n",
    "        epochs=10,\n",
    "        callbacks=[save_print_callback]\n",
    "    )\n",
    "\n",
    "# Save the model\n",
    "twin_model.save(\"/kaggle/working/twin_resnet_model.h5\")\n",
    "\n",
    "# Evaluate the Model\n",
    "with tf.device('/GPU:0'):\n",
    "    loss, accuracy = twin_model.evaluate(val_dataset, steps=validation_steps)\n",
    "    print(f\"🎯 Final Validation Accuracy: {accuracy:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
